{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement directement des vecteurs (si on ne veut pas tout recalculer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#vectors.to_pickle('./input/zero_padded_family_data.pkl')\n",
    "vectors= pd.read_pickle('./input/occurrence_family_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du jeu de données de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_json('./ressources/jeu_de_validation.txt', lines=True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge entre jeu de données et données de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.merge(vectors, how='inner',on='store_id')\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_presse (type):\n",
    "    if 'presse' in type:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "test_dataset['is_presse']= test_dataset.type.apply(is_presse)\n",
    "\n",
    "def is_tabac (type):\n",
    "    if 'tabac' in type:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "test_dataset['is_tabac']= test_dataset.type.apply(is_tabac)\n",
    "\n",
    "def is_restaurant(type):\n",
    "    if 'restaurant' in type:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "test_dataset['is_restaurant']= test_dataset.type.apply(is_restaurant)\n",
    "\n",
    "def is_hotel(type):\n",
    "    if 'hotel' in type:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "test_dataset['is_hotel']= test_dataset.type.apply(is_hotel)\n",
    "\n",
    "\n",
    "def is_essence(type):\n",
    "    if 'essence' in type:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "test_dataset['is_essence']= test_dataset.type.apply(is_essence)\n",
    "\n",
    "def is_bar(type):\n",
    "    if 'bar' in type:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "test_dataset['is_bar']= test_dataset.type.apply(is_bar)\n",
    "\n",
    "def is_epicerie(type):\n",
    "    if 'epicerie' in type:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "test_dataset['is_epicerie']= test_dataset.type.apply(is_epicerie)\n",
    "\n",
    "def is_boulangerie(type):\n",
    "    if 'epicerie' in type:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "test_dataset['is_boulangerie']= test_dataset.type.apply(is_boulangerie)\n",
    "\n",
    "def is_cafe(type):\n",
    "    if 'cafe' in type:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "test_dataset['is_cafe']= test_dataset.type.apply(is_cafe)\n",
    "\n",
    "\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des échantillons d'apprentissage et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# features = ['is_presse','is_tabac','is_restaurant','is_hotel','is_essence','is_bar','is_epicerie','is_boulangerie','is_cafe']\n",
    "features = ['is_presse','is_tabac','is_restaurant','is_hotel','is_bar','is_epicerie','is_boulangerie','is_cafe']\n",
    "\n",
    "\n",
    "\n",
    "X =np.concatenate([test_dataset.store_id.values.reshape(-1,1),scaler.fit_transform(np.vstack(test_dataset.vector.values).T).T],axis=1)\n",
    "\n",
    "#X =np.concatenate([test_dataset.store_id.values.reshape(-1,1),np.vstack(test_dataset.vector.values)],axis=1)\n",
    "\n",
    "y = test_dataset[features].values  #test_dataset.is_presse.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etiquetage multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "48 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "48 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/sklearn/multioutput.py\", line 434, in fit\n",
      "    super().fit(X, Y, sample_weight, **fit_params)\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/sklearn/multioutput.py\", line 202, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/joblib/parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/sklearn/multioutput.py\", line 44, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 199, in fit\n",
      "    y = self._validate_targets(y)\n",
      "  File \"/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 720, in _validate_targets\n",
      "    raise ValueError(\n",
      "ValueError: The number of classes has to be greater than one; got 1 class\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/mperrot456/Documents/dev/py-venv1/lib/python3.10/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = SVC()\n",
    "\n",
    "pipe = Pipeline(steps=[('clf', MultiOutputClassifier(clf))])\n",
    "search_space = {\n",
    "    'clf__estimator__C': [0.1,1, 10, 100],\n",
    "    'clf__estimator__gamma': [1,0.1,0.01,0.001],\n",
    "    'clf__estimator__kernel':['rbf', 'poly', 'sigmoid']    \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, search_space, scoring='accuracy')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# y_pred = multi_target_forest.fit(X_train[:,1:], y_train).predict(X_test[:,1:])\n",
    "# np.savetxt(\"z__y_pred_zeropads_RF.csv\", y_pred, delimiter=\";\")\n",
    "\n",
    "y_pred = grid_search.fit(X_train[:,1:], y_train).predict(X_test[:,1:])\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats de gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('clf', MultiOutputClassifier(estimator=SVC(C=0.1, gamma=1)))])\n",
      "nan\n",
      "{'clf__estimator__C': 0.1, 'clf__estimator__gamma': 1, 'clf__estimator__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'clf__estimator__C': 0.1, 'clf__estimator__gamma': 1, 'clf__estimator__kernel': 'rbf'}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b181ddea01f9d81e2d95603baae4de9d1704dec5878612b056bc14d59a790c8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('py-venv1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58a1850f5dfb469d7399ed11ad3e7ae36a2688590636f88e3038dd0fc73e7401"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
